{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def camera_calibration():\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((6*9,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "    # Step through the list and search for chessboard corners\n",
    "    for fname in images:\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "    return objpoints, imgpoints\n",
    "\n",
    "def undist_img(img, objpoints, imgpoints):\n",
    "    #img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[1::-1], None, None)\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist\n",
    "\n",
    "### Define function to help show image outputs ####\n",
    "def show_test_images(og_img, ud_img, string1 = 'Original Image', string2 = 'Undistored Image'):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(og_img, cmap='gray')\n",
    "    ax1.set_title(string1, fontsize=50)\n",
    "    ax2.imshow(ud_img, cmap='gray')\n",
    "    ax2.set_title(string2, fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "def region_of_interest(img):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    `vertices` should be a numpy array of integer points.\n",
    "    \"\"\"\n",
    "    height_half = img.shape[0]/2\n",
    "    width_half = img.shape[1]/2\n",
    "    \n",
    "    left_bottom = [0, img.shape[0]]\n",
    "    left_top = [width_half*0.85, height_half*1.25]\n",
    "    right_bottom = [img.shape[1], img.shape[0]]\n",
    "    right_top = [width_half*1.15, height_half*1.25]\n",
    "    \n",
    "    vertices = np.array([[left_bottom],[left_top],[right_top],[right_bottom]], np.int32)\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, [vertices], ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    \n",
    "    return masked_image\n",
    "\n",
    "def abs_sobel_thresh(img, orient='x'):\n",
    "    #Convert to gray scale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if orient == 'x':\n",
    "        thresh=(20,100)\n",
    "        abs_sobelx = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5))\n",
    "        #Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "        scaled_sobelx = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "        #Create a mask of 1's where the scaled gradient magnitude \n",
    "            # is > thresh_min and < thresh_max\n",
    "        sxbin = np.zeros_like(scaled_sobelx)\n",
    "        sxbin[(scaled_sobelx > thresh[0]) & (scaled_sobelx < thresh[1])] = 1\n",
    "        sobel_output = sxbin\n",
    "    elif orient == 'y':\n",
    "        thresh=(40,100)\n",
    "        abs_sobely = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1))\n",
    "        #Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "        scaled_sobely = np.uint8(255*abs_sobely/np.max(abs_sobely))\n",
    "        #Create a mask of 1's where the scaled gradient magnitude \n",
    "            # is > thresh_min and < thresh_max\n",
    "        sybin = np.zeros_like(scaled_sobely)\n",
    "        sybin[(scaled_sobely > thresh[0]) & (scaled_sobely < thresh[1])] = 1\n",
    "        sobel_output = sybin\n",
    "    \n",
    "    return sobel_output\n",
    "                        \n",
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(0,255)):\n",
    "    #Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    #Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    #Calculate the magnitude \n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    #Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scale_factor = np.max(gradmag)/255\n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8)\n",
    "    #scaled_mag= np.uint8(255*gradmag/np.max(gradmag))\n",
    "    #Create a binary mask where mag thresholds are met\n",
    "    mag_bin = np.zeros_like(gradmag)\n",
    "    mag_bin[(gradmag > mag_thresh[0]) & (gradmag < mag_thresh[1])] = 1\n",
    "    #Return this mask as binary_output image \n",
    "    return mag_bin\n",
    "\n",
    "def dir_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    \n",
    "    # Apply the following steps to img    \n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    # 3) Take the absolute value of the x and y gradients\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    # 4) Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient\n",
    "    sobel_direc = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    # 5) Create a binary mask where direction thresholds are met\n",
    "    direc_bin = np.zeros_like(sobel_direc)\n",
    "    direc_bin[(sobel_direc > thresh[0]) & (sobel_direc < thresh[1])] = 1\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    return direc_bin\n",
    "\n",
    "def hls_select(img, channel='s', thresh=(0, 255)):\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    if channel == 'h':\n",
    "        h_channel = hls[:,:,0]\n",
    "        binary_output = np.zeros_like(h_channel)\n",
    "        binary_output[(h_channel > thresh[0]) & (h_channel <= thresh[1])] = 1\n",
    "    elif channel == 'l':\n",
    "        l_channel = hls[:,:,1]\n",
    "        binary_output = np.zeros_like(l_channel)\n",
    "        binary_output[(l_channel > thresh[0]) & (l_channel <= thresh[1])] = 1\n",
    "    elif channel == 's':\n",
    "        s_channel = hls[:,:,2]\n",
    "        binary_output = np.zeros_like(s_channel)\n",
    "        binary_output[(s_channel > thresh[0]) & (s_channel <= thresh[1])] = 1\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "\n",
    "def r_colorspace(img, thresh=(200,255)):\n",
    "    r_img = img[:,:,0]\n",
    "    binary_output = np.zeros_like(r_img)\n",
    "    binary_output[(r_img>thresh[0])& (r_img<thresh[1])]=1\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "def combined_threshold(img):\n",
    "    #combining thresholds of sobel_x, S channel, \n",
    "    sobel_x = abs_sobel_thresh(img, orient='x')\n",
    "    sobel_y = abs_sobel_thresh(img, orient='y')\n",
    "    s_color = hls_select(img, channel='s',thresh=(120,255))\n",
    "    R_color= r_colorspace(img, thresh=(220,255))\n",
    "\n",
    "    combined_bin = np.zeros_like(sobel_x)\n",
    "    combined_bin[((sobel_x==1) & (s_color==1))| (R_color==1)]=1\n",
    "    #combined_bin[(sobel_x==1) | ((s_color==1) & (R_color==1))]=1\n",
    "    #combined_bin[((sobel_x==1) & (sobel_y ==1))| ((s_color==1)& (R_color==1))]=1\n",
    "    #stack each channel for visual of each binary\n",
    "    color_bin = np.dstack((np.zeros_like(sobel_x), sobel_x, s_color)) * 255\n",
    "    \n",
    "    return combined_bin\n",
    "\n",
    "def bird_eye_transform(img, orient='M'):\n",
    "    \n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "\n",
    "    src_points = np.float32(\n",
    "    [[280,  720],\n",
    "     [595,  450],\n",
    "     [725,  450],\n",
    "     [1125, 720]])\n",
    "\n",
    "    dst_points = np.float32(\n",
    "    [[250,   720],\n",
    "     [250,     0],\n",
    "     [1000,    0],\n",
    "     [1000,  720]])\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "    M_inv = cv2.getPerspectiveTransform(dst_points, src_points)\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    if orient == 'M_inv':\n",
    "        warped = cv2.warpPerspective(img, M_inv, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    #return warped, M, M_inv\n",
    "    return warped\n",
    "\n",
    "def find_lane_pixels(binary_warped, show_img=False):\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)\n",
    "    \n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    \n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "        \n",
    "    nwindows = 14\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 150\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 100\n",
    "    \n",
    "    # Set height of windows - based on nwindows above and image shape\n",
    "    window_height = np.int(binary_warped.shape[0]//nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated later for each window in nwindows\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    left_x_coords = []\n",
    "    right_x_coords =[]\n",
    "    count = -1\n",
    "    count_left = -1\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        ### TO-DO: Find the four below boundaries of the window ###\n",
    "        win_xleft_low = leftx_current - margin//2  # Update this\n",
    "        win_xleft_high = leftx_current + margin//2 # Update this\n",
    "        win_xright_low = rightx_current - margin//2  # Update this\n",
    "        win_xright_high = rightx_current + margin//2  # Update this\n",
    "        \n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),\n",
    "        (win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),\n",
    "        (win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        \n",
    "        #Identify the nonzero pixels in x and y within the window ###\n",
    "        good_left_inds = ((nonzerox > win_xleft_low) & (nonzerox < win_xleft_high) \n",
    "        & (nonzeroy > win_y_low) & (nonzeroy < win_y_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzerox > win_xright_low) & (nonzerox < win_xright_high) \n",
    "        & (nonzeroy > win_y_low) & (nonzeroy < win_y_high)).nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        #If you found > minpix pixels, recenter next window ###\n",
    "        ### (`right` or `leftx_current`) on their mean position ###\n",
    "        #### else if need to follow along the slow of the previous x position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "            left_x_coords.append(leftx_current)\n",
    "            count_left +=1\n",
    "        elif(count_left > 0 and len(left_x_coords) > 1):\n",
    "            if left_x_coords[count_left] != left_x_coords[count_left-1]:\n",
    "                left_slope = (win_y_high - (win_y_high - window_height))//(left_x_coords[count_left] - left_x_coords[count_left-1])\n",
    "                b_int_left = (win_y_high-window_height) - left_slope*left_x_coords[count_left]\n",
    "                leftx_current = (win_y_high-b_int_left)//left_slope\n",
    "                left_x_coords.append(leftx_current)\n",
    "                count_left +=1\n",
    "        if len(good_right_inds) > minpix:\n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "            right_x_coords.append(rightx_current)\n",
    "            count +=1\n",
    "        elif(count > 0 and len(right_x_coords)> 1):\n",
    "            if right_x_coords[count] != right_x_coords[count-1]:\n",
    "                right_slope = (win_y_high - (win_y_high-window_height))//(right_x_coords[count]-right_x_coords[count-1])\n",
    "                b_int_right = (win_y_high-window_height) - right_slope*right_x_coords[count]\n",
    "                rightx_current = (win_y_high - b_int_right)//right_slope\n",
    "            right_x_coords.append(rightx_current)\n",
    "            count +=1\n",
    "    \n",
    "    # Concatenate the arrays of indices (previously was a list of lists of pixels)\n",
    "    try:\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    except ValueError:\n",
    "        # Avoids an error if the above is not implemented fully\n",
    "        pass\n",
    "    \n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    #Fit a second order polynomial to each using `np.polyfit` ###\n",
    "    try:\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "    except:\n",
    "        ('print empty values found')\n",
    "        pass\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    \n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    # package line fits, left points, and right points into seperate lines\n",
    "    poly_fits = (left_fit, right_fit)\n",
    "    left_pixs = (leftx, lefty)\n",
    "    right_pixs = (rightx, righty)\n",
    "    \n",
    "    if show_img == True:\n",
    "        #for x in range(binary_warped.shape[0]):\n",
    "            # Color in left and right line pixels\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "        left_pts = np.array(np.dstack((left_fitx, ploty)), np.int32)\n",
    "        right_pts = np.array(np.dstack((right_fitx, ploty)), np.int32)\n",
    "        cv2.polylines(out_img, left_pts, False, (255,0,0), 5)\n",
    "        cv2.polylines(out_img, right_pts, False, (255,0,0), 5)\n",
    "        \n",
    "        return poly_fits, out_img\n",
    "    \n",
    "    return poly_fits\n",
    "\n",
    "def search_around_poly(binary_warped, poly_fits=None, show_img=False):\n",
    "    #Determine if there are previous polyfit lines:\n",
    "    if poly_fits is None:\n",
    "        print('Finding Pixels')\n",
    "        poly_fits = find_lane_pixels(binary_warped)\n",
    "        \n",
    "    left_fit = poly_fits[0]\n",
    "    right_fit = poly_fits[1]\n",
    "\n",
    "    margin = 75\n",
    "\n",
    "    # Grab activated pixels\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    left_lane_inds = ((nonzerox > left_fit[0]*nonzeroy**2 + left_fit[1]*nonzeroy + left_fit[2]-margin) \n",
    "    & (nonzerox < left_fit[0]*nonzeroy**2 + left_fit[1]*nonzeroy + left_fit[2] + margin))\n",
    "    right_lane_inds = ((nonzerox > right_fit[0]*nonzeroy**2 + right_fit[1]*nonzeroy + right_fit[2] - margin)\n",
    "    & (nonzerox < right_fit[0]*nonzeroy**2 + right_fit[1]*nonzeroy+right_fit[2]+margin))\n",
    "    \n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    \n",
    "    # Fit new polynomials\n",
    "    try:\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "    except:\n",
    "        print('empty values found. Performing search again')\n",
    "        poly_fits = find_lane_pixels(binary_warped)\n",
    "        left_fit = poly_fits[0]\n",
    "        right_fit = poly_fits[1]\n",
    "        \n",
    "    ploty = np.linspace(0, binary_warped.shape[0] -1, binary_warped.shape[0])\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    ## Visualization ##\n",
    "    # Create an image to draw on and an image to show the selection window\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    window_img = np.zeros_like(out_img)\n",
    "    # Color in left and right line pixels\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "    # Generate a polygon to illustrate the search window area\n",
    "    # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "    left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "    left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, \n",
    "                              ploty])))])\n",
    "    left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "    right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "    right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, \n",
    "                              ploty])))])\n",
    "    right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "    cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "    result_img = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "    \n",
    "    #Package lines \n",
    "    poly_fits = (left_fit, right_fit)\n",
    "    #left_pix = (leftx, lefty)\n",
    "    #right_pix = (rightx, righty)\n",
    "    left_line = (left_fitx, ploty)\n",
    "    right_line = (right_fitx, ploty)\n",
    "    # Plot the polynomial lines onto the image\n",
    "\n",
    "    #plt.plot(left_fitx, ploty, color='yellow')\n",
    "    #plt.plot(right_fitx, ploty, color='yellow')\n",
    "    ## End visualization steps ##\n",
    "    \n",
    "    return poly_fits, left_line, right_line, result_img\n",
    "\n",
    "def measure_curvature_pixels(left_line_ind, right_line_ind):\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    \n",
    "    leftx = left_line_ind[0]\n",
    "    rightx = right_line_ind[0]\n",
    "    ploty = right_line_ind[1]\n",
    "    \n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    leftx = leftx[::-1]  # Reverse to match top-to-bottom in y\n",
    "    rightx = rightx[::-1]  # Reverse to match top-to-bottom in y\n",
    "    \n",
    "    left_fit_curve = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix,2)\n",
    "    right_fit_curve = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix,2)\n",
    "\n",
    "    # Calculation of R_curve (radius of curvature)\n",
    "    left_curverad = ((1 + (2*left_fit_curve[0]*y_eval*ym_per_pix + left_fit_curve[1])**2)**1.5) / np.absolute(2*left_fit_curve[0])\n",
    "    right_curverad = ((1 + (2*right_fit_curve[0]*y_eval*ym_per_pix + right_fit_curve[1])**2)**1.5) / np.absolute(2*right_fit_curve[0])\n",
    "    \n",
    "    laneCurve = (left_curverad+right_curverad)/2\n",
    "    roundCurve = math.ceil(laneCurve/500)*500\n",
    "    if roundCurve > 10000:\n",
    "        roundCurve = 10000\n",
    "        \n",
    "    return roundCurve\n",
    "\n",
    "def find_dist_lane(img, left_line_ind, right_line_ind):\n",
    "    \n",
    "    leftx = left_line_ind[0]\n",
    "    rightx = right_line_ind[0]\n",
    "    y = right_line_ind[1]\n",
    "    \n",
    "    leftx_pts = np.flipud(np.transpose(np.vstack([leftx,y])))\n",
    "    rightx_pts = np.flipud(np.transpose(((np.vstack([rightx, y])))))\n",
    "    \n",
    "    ym_per_pix = 30/720\n",
    "    xm_per_pix = 3.7/700\n",
    "    \n",
    "    f_leftx, f_lefty = leftx_pts[0]\n",
    "    f_rightx, f_righty = rightx_pts[0]\n",
    "    \n",
    "    mid_img = img.shape[1]//2\n",
    "    car_pos = (f_leftx+f_rightx)/2\n",
    "    \n",
    "    distToCent = round(((mid_img - car_pos)*xm_per_pix),3)\n",
    "\n",
    "    \n",
    "    return distToCent\n",
    "\n",
    "def OG_PolyPoints(img, perspective_img, left_line_ind, right_line_ind):    \n",
    "    #need to transpose and vstack since points go from top to bottom\n",
    "    left_pts = np.array([np.transpose(np.vstack([left_line_ind]))]).astype(np.int32)\n",
    "    #need to transpose, vstack and flip updown since right bottom to top\n",
    "    right_pts = np.array([np.flipud(np.transpose(np.vstack([right_line_ind])))]).astype(np.int32)\n",
    "    all_pts = np.hstack((left_pts, right_pts))\n",
    "    \n",
    "    overlay = np.zeros_like(perspective_img, dtype=np.uint8)\n",
    "    color_overlay = np.dstack((overlay, overlay, overlay))\n",
    "    \n",
    "    cv2.fillPoly(color_overlay, np.int_([all_pts]), (0,255,0))\n",
    "    cv2.polylines(color_overlay, left_pts, isClosed=False, color=(255,0,0), thickness=30)\n",
    "    cv2.polylines(color_overlay, right_pts, isClosed=False, color=(255,0,0), thickness=30)  \n",
    "    \n",
    "    #unwarped_overlay = cv2.warpPerspective(color_overlay, M_inv, (img.shape[1], img.shape[0]))\n",
    "    unwarped_overlay = bird_eye_transform(color_overlay, orient='M_inv')\n",
    "    OG_overlay_img = cv2.addWeighted(img, 1, unwarped_overlay, 0.3, 0)\n",
    "    \n",
    "    return OG_overlay_img\n",
    "    \n",
    "def TextInfo(img, dist, curvature): \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text = \"Vehicle distance to center: {} m\".format(dist)\n",
    "    cv2.putText(img,text, (600,50), font, 1, (255,255,255), 2)\n",
    "    text2 = \"Lane curvature: {} m \".format(int(curvature))\n",
    "    cv2.putText(img,text2, (600,100), font, 1, (255,255,255), 2)\n",
    "    return img\n",
    "\n",
    "# Defining a class to make camera calibration seperate from the image processing pipeline\n",
    "class Pipeline: \n",
    "    #Calibrate camera when creating new object\n",
    "    def __init__(self):\n",
    "        #initialize camera calibration\n",
    "        self.objpoints, self.imgpoints = camera_calibration()\n",
    "        #initialize empty lines_fit\n",
    "        self.poly_fits = None\n",
    "    \"\"\" use call function to be able to call the class as a function, this allows the camera calibration to only happen\n",
    "        at the creation of a new object and not every iteration of the image process pipeline. \"\"\"\n",
    "    def __call__(self, img):\n",
    "        undist = undist_img(img, self.objpoints, self.imgpoints)\n",
    "\n",
    "        x_sobel = abs_sobel_thresh(undist, orient='x')\n",
    "\n",
    "        combined_bin = combined_threshold(undist)\n",
    "\n",
    "        roi_img = region_of_interest(combined_bin)\n",
    "\n",
    "        persp_img = bird_eye_transform(roi_img, orient='M')\n",
    "\n",
    "        self.poly_fits, left_line, right_line, out_img = search_around_poly(persp_img, self.poly_fits)\n",
    "\n",
    "        curvature = measure_curvature_pixels(left_line, right_line)\n",
    "\n",
    "        distance = find_dist_lane(persp_img, left_line, right_line)\n",
    "\n",
    "        image_lane = OG_PolyPoints(undist, persp_img, left_line, right_line)\n",
    "\n",
    "        final_img = TextInfo(image_lane, distance, curvature)\n",
    "\n",
    "        return final_img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Pixels\n",
      "[MoviePy] >>>> Building video challenge_video_output.mp4\n",
      "[MoviePy] Writing video challenge_video_output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/300 [00:01<06:21,  1.28s/it]\u001b[A\n",
      "  1%|          | 2/300 [00:02<06:21,  1.28s/it]\u001b[A\n",
      "  1%|          | 3/300 [00:03<06:20,  1.28s/it]\u001b[A\n",
      "  1%|▏         | 4/300 [00:05<06:18,  1.28s/it]\u001b[A\n",
      "  2%|▏         | 5/300 [00:06<06:16,  1.28s/it]\u001b[A\n",
      "  2%|▏         | 6/300 [00:07<06:15,  1.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty values found. Performing search again\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'right_fit' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a6dcfc8efc80>\u001b[0m in \u001b[0;36msearch_around_poly\u001b[0;34m(binary_warped, poly_fits, show_img)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mleft_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlefty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleftx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mright_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrighty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrightx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/polynomial.py\u001b[0m in \u001b[0;36mpolyfit\u001b[0;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected non-empty vector for x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected non-empty vector for x",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-175>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-174>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    135\u001b[0m              for (k,v) in k.items()}\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-173>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[1;32m    347\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                            \u001b[0mffmpeg_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mffmpeg_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                            progress_bar=progress_bar)\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     for t,frame in clip.iter_frames(progress_bar=progress_bar, with_times=True,\n\u001b[0;32m--> 209\u001b[0;31m                                     fps=fps, dtype=\"uint8\"):\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwithmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                 \u001b[0;31m# Update and print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-138>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0manother\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a6dcfc8efc80>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mpersp_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbird_eye_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroi_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoly_fits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_around_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersp_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoly_fits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mcurvature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure_curvature_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a6dcfc8efc80>\u001b[0m in \u001b[0;36msearch_around_poly\u001b[0;34m(binary_warped, poly_fits, show_img)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'empty values found. Performing search again'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mpoly_fits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_lane_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_warped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mleft_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly_fits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mright_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly_fits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a6dcfc8efc80>\u001b[0m in \u001b[0;36mfind_lane_pixels\u001b[0;34m(binary_warped, show_img)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mleft_fitx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mploty\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mleft_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mploty\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mleft_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mright_fitx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mright_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mploty\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mright_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mploty\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mright_fit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;31m# package line fits, left points, and right points into seperate lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'right_fit' referenced before assignment"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "white_output = 'challenge_video_output.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "pipeline_image = Pipeline()\n",
    "clip1 = VideoFileClip(\"challenge_video.mp4\").subclip(0,10)\n",
    "#clip1 = VideoFileClip('challenge_video.mp4')\n",
    "white_clip = clip1.fl_image(pipeline_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
